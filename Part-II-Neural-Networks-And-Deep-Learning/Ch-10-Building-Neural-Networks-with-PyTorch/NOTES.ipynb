{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32417b24",
   "metadata": {},
   "source": [
    "# Chapter 10. Building Neural Networks with PyTorch\n",
    "\n",
    "PyTorch is an open-source deep learning library for working with machine learning models. It's goal was to provide a pythonic interface for researchers, originally developed by Meta and now under the governance of the PyTorch foundation.\n",
    "\n",
    "## Fundamentals\n",
    "\n",
    "The fundamental core data structure in PyTorch is the *tensor* which is a multidimensional array with a shape and a data type not so unlike a NumPy array. It will become the input and output of our neural networks just like NumPy arrays were in Sciki-Learn models.\n",
    "\n",
    "### PyTorch Tensors\n",
    "\n",
    "Import the library and create a 2 x 3 array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e91468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac33009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.0, 4.0, 7.0], [2.0, 3.0, 6.0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d622c6",
   "metadata": {},
   "source": [
    "Getting shape and data type is similar to NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6278e5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1647f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca282b9",
   "metadata": {},
   "source": [
    "So is indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca2d24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a73b93ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 3.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b418c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20., 50., 80.],\n",
       "        [30., 40., 70.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 * (X + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2bdff5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2.7183,   54.5982, 1096.6332],\n",
       "        [   7.3891,   20.0855,  403.4288]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1157e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8333)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0ed3cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([2., 4., 7.]),\n",
       "indices=tensor([1, 0, 0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25457f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([7., 6.]),\n",
       "indices=tensor([2, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c89e2811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[66., 56.],\n",
       "        [56., 49.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X @ X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753baa5",
   "metadata": {},
   "source": [
    "The `numpy()` method enables creating a tensor from a NumPy array and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d17890b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X.numpy()\n",
    "\n",
    "torch.tensor(np.array([[1., 4., 7.], [2. ,3., 6.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea562b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., -99.,   7.],\n",
       "        [  2., -99.,   6.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 1] = -99\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bedc1a2",
   "metadata": {},
   "source": [
    "QUESTION: what does it mean that these api methods with the underscore apply operations in-place? Does that mean without storing a copy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc4d0bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 7.],\n",
       "        [2., 0., 6.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.relu_()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f40cc",
   "metadata": {},
   "source": [
    "### Hardware Acceleration\n",
    "\n",
    "One benefit of PyTorch is the hardware acceleration that greatly speeds up computations. Unlike SciKit Learn, we can choose from using NVIDIA GPUs with CUDA, Apple's *Metal Perfomance Shaders* (MPS), AMD ROCm, Intel's oneAPI, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9fd64a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfdbf3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0939689",
   "metadata": {},
   "source": [
    "Creating a Tensor on the CPU then copying it to the accelerator `device` with the `to()` method.\n",
    "The tensor's `device` attribute will show the device it livs on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7713e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
    "M = M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c80c5400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb41db1",
   "metadata": {},
   "source": [
    "The tensor can also be created directly on the GPU using the `device` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b1d0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0ae75e",
   "metadata": {},
   "source": [
    "QUESTION: why isn't this the default? what's the scenario for either use case? Maybe if we want to do some easy manipulations and data verification on the tensor in CPU to keep the GPU freed up for the more intense training computations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d05dda8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14., 32.],\n",
       "        [32., 77.]], device='mps:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = M @ M.T\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16c01e",
   "metadata": {},
   "source": [
    "Crucially the resulting Tensor `R` also lives on the accelerator device so we are saved the bottleneck of data transfer between devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "876c8e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.46 ms ± 160 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000, 1000))\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce4d04cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 μs ± 2.68 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000, 1000), device=device)\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17297a00",
   "metadata": {},
   "source": [
    "Interestinglly the `mpu` accelerated tensor operation was significnatly faster like an order of magnitude faster. Honestly the cpu operation was not slouch probably due to the Apple M4's ARM architecture and unified memory. \n",
    "\n",
    "It's curious to me that the actual python execution of the accelerated block took a lot longer. Maybe this was just a temporary Jupyter kernel quirk or the increased overhead of communicating the process back to the python kernel running in the cpu caused that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65880c",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "The reverse-mode auto-differentiation implementation in PyTorch is *autograd* automatic gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb9220ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "f = x ** 2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2defb597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba4b0af",
   "metadata": {},
   "source": [
    "A breakdown of this code is as follows. \n",
    "\n",
    "First a tensor is created, `x` of value `5.0`. In order for it to not be treated as a constant value we provide `reuiqres_grad=True` so that PyTorch internally will track operations performed on it (necessary for our autograd).\n",
    "\n",
    "The definition of our f(x) is fairly straightforward we are performing an exponential operaion on the tensor x. This gives us a new tensor of the resulting value `25.` and the `grad_fn=<PowBackward0>` which is the operation we just performed, creating the relationship between this tensor and the one it is modified from, `x`. This `grad_fn` attribute keeps track of the *computation graph*.\n",
    "\n",
    "Given this, when we call `f.backward()` pytorch is traversing this graph backwards starting at `f` calculating the gradients all the way back to the leaf nodes which in this case is just `x`.\n",
    "\n",
    "This then allows us to read the tensor `x`'s gradient with `x.grad`. If we would have run this at the beginning it's gradient would be empty. This gradient was computed during the backprop and gives us the derivative of `f` with regard to `x`. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f5bb7",
   "metadata": {},
   "source": [
    "For gradient descent, the reduction operation subtracting a fraction of the gradients from the model variables should not be tracked and in fact raises an exception in PyTorch. To exclude these from the gradient descent steps from the computation graph, we can place each step insdie a `torch.no_grad()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc4ded88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 3.0, x.grad 10.0\n",
      "x: 2.0, x.grad 10.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "print(f\"x: {x}, x.grad {x.grad}\")\n",
    "with torch.no_grad():\n",
    "    x -= learning_rate * x.grad\n",
    "print(f\"x: {x}, x.grad {x.grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d69d6",
   "metadata": {},
   "source": [
    "You can also avoid gradient computation by using the `detach()` method creating a new tensor detached from the computation graph, with `requires_grad=False` but pointing to the same data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3547fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 3.0, x.grad 10.0\n",
      "x_detached: 3.0, x_detached.grad None\n"
     ]
    }
   ],
   "source": [
    "x_detached = x.detach()\n",
    "x_detached -= learning_rate * x.grad\n",
    "print(f\"x: {x}, x.grad {x.grad}\")\n",
    "print(f\"x_detached: {x_detached}, x_detached.grad {x_detached.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6a200",
   "metadata": {},
   "source": [
    "So `detach()` is handy for performing operations that you don't want affecting the gradient, for logging or other things, and since `x_detached` and `x` share the same memory, modifying `x_detached` also modifies `x`. In general `no_grad()` is preferred when performing inference or doing a gradient descent step. \n",
    "\n",
    "Before repeating the process the gradients of every model parameter need to be zeroed out. The gradient tensor has `required_grad=False` so a `no_grad()` is not necessary. (Yes I guess the grad_fn is also a tensor?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33e0e21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ab126a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "for i in range(100):\n",
    "    f = x ** 2  # forward pass\n",
    "    f.backward()    # backward pass\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "\n",
    "    x.grad.zero_()  # zero out the gradients at the end of each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478b51a",
   "metadata": {},
   "source": [
    "> in place operations save space by reducing intermediate copies but this doesn't work well with autograd where they are needed to perform the backward pass. Instead of `z += 1` instead use `z = z + 1`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

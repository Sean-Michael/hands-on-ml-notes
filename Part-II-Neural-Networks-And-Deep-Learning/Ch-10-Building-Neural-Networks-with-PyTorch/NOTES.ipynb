{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32417b24",
   "metadata": {},
   "source": [
    "# Chapter 10. Building Neural Networks with PyTorch\n",
    "\n",
    "PyTorch is an open-source deep learning library for working with machine learning models. It's goal was to provide a pythonic interface for researchers, originally developed by Meta and now under the governance of the PyTorch foundation.\n",
    "\n",
    "## Fundamentals\n",
    "\n",
    "The fundamental core data structure in PyTorch is the *tensor* which is a multidimensional array with a shape and a data type not so unlike a NumPy array. It will become the input and output of our neural networks just like NumPy arrays were in Sciki-Learn models.\n",
    "\n",
    "### PyTorch Tensors\n",
    "\n",
    "Import the library and create a 2 x 3 array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e91468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac33009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.0, 4.0, 7.0], [2.0, 3.0, 6.0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d622c6",
   "metadata": {},
   "source": [
    "Getting shape and data type is similar to NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6278e5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1647f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca282b9",
   "metadata": {},
   "source": [
    "So is indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca2d24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73b93ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 3.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b418c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20., 50., 80.],\n",
       "        [30., 40., 70.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 * (X + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2bdff5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2.7183,   54.5982, 1096.6332],\n",
       "        [   7.3891,   20.0855,  403.4288]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1157e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8333)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0ed3cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([2., 4., 7.]),\n",
       "indices=tensor([1, 0, 0]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25457f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([7., 6.]),\n",
       "indices=tensor([2, 2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c89e2811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[66., 56.],\n",
       "        [56., 49.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X @ X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753baa5",
   "metadata": {},
   "source": [
    "The `numpy()` method enables creating a tensor from a NumPy array and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d17890b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X.numpy()\n",
    "\n",
    "torch.tensor(np.array([[1., 4., 7.], [2. ,3., 6.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea562b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., -99.,   7.],\n",
       "        [  2., -99.,   6.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 1] = -99\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bedc1a2",
   "metadata": {},
   "source": [
    "QUESTION: what does it mean that these api methods with the underscore apply operations in-place? Does that mean without storing a copy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc4d0bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 7.],\n",
       "        [2., 0., 6.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.relu_()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f40cc",
   "metadata": {},
   "source": [
    "### Hardware Acceleration\n",
    "\n",
    "One benefit of PyTorch is the hardware acceleration that greatly speeds up computations. Unlike SciKit Learn, we can choose from using NVIDIA GPUs with CUDA, Apple's *Metal Perfomance Shaders* (MPS), AMD ROCm, Intel's oneAPI, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9fd64a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfdbf3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0939689",
   "metadata": {},
   "source": [
    "Creating a Tensor on the CPU then copying it to the accelerator `device` with the `to()` method.\n",
    "The tensor's `device` attribute will show the device it livs on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7713e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
    "M = M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c80c5400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb41db1",
   "metadata": {},
   "source": [
    "The tensor can also be created directly on the GPU using the `device` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b1d0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0ae75e",
   "metadata": {},
   "source": [
    "QUESTION: why isn't this the default? what's the scenario for either use case? Maybe if we want to do some easy manipulations and data verification on the tensor in CPU to keep the GPU freed up for the more intense training computations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d05dda8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14., 32.],\n",
       "        [32., 77.]], device='mps:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = M @ M.T\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16c01e",
   "metadata": {},
   "source": [
    "Crucially the resulting Tensor `R` also lives on the accelerator device so we are saved the bottleneck of data transfer between devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "876c8e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.34 ms ± 74.1 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000, 1000))\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce4d04cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 μs ± 3.25 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000, 1000), device=device)\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17297a00",
   "metadata": {},
   "source": [
    "Interestinglly the `mpu` accelerated tensor operation was significnatly faster like an order of magnitude faster. Honestly the cpu operation was not slouch probably due to the Apple M4's ARM architecture and unified memory. \n",
    "\n",
    "It's curious to me that the actual python execution of the accelerated block took a lot longer. Maybe this was just a temporary Jupyter kernel quirk or the increased overhead of communicating the process back to the python kernel running in the cpu caused that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65880c",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "The reverse-mode auto-differentiation implementation in PyTorch is *autograd* automatic gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb9220ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "f = x ** 2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2defb597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba4b0af",
   "metadata": {},
   "source": [
    "A breakdown of this code is as follows. \n",
    "\n",
    "First a tensor is created, `x` of value `5.0`. In order for it to not be treated as a constant value we provide `reuiqres_grad=True` so that PyTorch internally will track operations performed on it (necessary for our autograd).\n",
    "\n",
    "The definition of our f(x) is fairly straightforward we are performing an exponential operaion on the tensor x. This gives us a new tensor of the resulting value `25.` and the `grad_fn=<PowBackward0>` which is the operation we just performed, creating the relationship between this tensor and the one it is modified from, `x`. This `grad_fn` attribute keeps track of the *computation graph*.\n",
    "\n",
    "Given this, when we call `f.backward()` pytorch is traversing this graph backwards starting at `f` calculating the gradients all the way back to the leaf nodes which in this case is just `x`.\n",
    "\n",
    "This then allows us to read the tensor `x`'s gradient with `x.grad`. If we would have run this at the beginning it's gradient would be empty. This gradient was computed during the backprop and gives us the derivative of `f` with regard to `x`. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f5bb7",
   "metadata": {},
   "source": [
    "For gradient descent, the reduction operation subtracting a fraction of the gradients from the model variables should not be tracked and in fact raises an exception in PyTorch. To exclude these from the gradient descent steps from the computation graph, we can place each step insdie a `torch.no_grad()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc4ded88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 5.0, x.grad 10.0\n",
      "x: 4.0, x.grad 10.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "print(f\"x: {x}, x.grad {x.grad}\")\n",
    "with torch.no_grad():\n",
    "    x -= learning_rate * x.grad\n",
    "print(f\"x: {x}, x.grad {x.grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d69d6",
   "metadata": {},
   "source": [
    "You can also avoid gradient computation by using the `detach()` method creating a new tensor detached from the computation graph, with `requires_grad=False` but pointing to the same data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3547fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 3.0, x.grad 10.0\n",
      "x_detached: 3.0, x_detached.grad None\n"
     ]
    }
   ],
   "source": [
    "x_detached = x.detach()\n",
    "x_detached -= learning_rate * x.grad\n",
    "print(f\"x: {x}, x.grad {x.grad}\")\n",
    "print(f\"x_detached: {x_detached}, x_detached.grad {x_detached.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6a200",
   "metadata": {},
   "source": [
    "So `detach()` is handy for performing operations that you don't want affecting the gradient, for logging or other things, and since `x_detached` and `x` share the same memory, modifying `x_detached` also modifies `x`. In general `no_grad()` is preferred when performing inference or doing a gradient descent step. \n",
    "\n",
    "Before repeating the process the gradients of every model parameter need to be zeroed out. The gradient tensor has `required_grad=False` so a `no_grad()` is not necessary. (Yes I guess the grad_fn is also a tensor?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33e0e21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ab126a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "for i in range(100):\n",
    "    f = x ** 2  # forward pass\n",
    "    f.backward()    # backward pass\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "\n",
    "    x.grad.zero_()  # zero out the gradients at the end of each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478b51a",
   "metadata": {},
   "source": [
    "> in place operations save space by reducing intermediate copies but this doesn't work well with autograd where they are needed to perform the backward pass. Instead of `z += 1` instead use `z = z + 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792152ed",
   "metadata": {},
   "source": [
    "## Implementing Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0884acc",
   "metadata": {},
   "source": [
    "We'll utilize the same california housing dataset to train an NLP on linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c38b21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b435d9c",
   "metadata": {},
   "source": [
    "After downloading the dataset we split into a training and test set. Then we split the training set into a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1f8fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_test, y, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf9e52c",
   "metadata": {},
   "source": [
    "With our data downloaded and split into training and testing sets, we can normalize it. But this time, with Tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1877c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(x_train)\n",
    "x_valid = torch.FloatTensor(x_valid)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "means = x_train.mean(dim=0, keepdims=True)\n",
    "stds = x_train.std(dim=0, keepdims=True)\n",
    "x_train = (x_train - means) / stds\n",
    "x_valid = (x_valid - means) / stds\n",
    "x_test = (x_test - means) / stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331ef54",
   "metadata": {},
   "source": [
    "The targets should also be converted to tensors and need to be reshaped by adding a second dimension of size 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23413169",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.FloatTensor(y_train).reshape(-1, 1)\n",
    "y_valid = torch.FloatTensor(y_valid).reshape(-1, 1)\n",
    "y_test = torch.FloatTensor(y_test).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c44961",
   "metadata": {},
   "source": [
    "QUESTION: exactly how did we know to do that should probably get some better intuition of that I'm assuming this reshape is doing somekind of multiplication / dot product operation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab1608b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "n_features = x_train.shape[1]\n",
    "w = torch.randn((n_features, 1), requires_grad=True)\n",
    "b = torch.tensor(0., requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13e35dd",
   "metadata": {},
   "source": [
    "This created a weights `w` and bias `b` parameter with randomly initialized weights and the bias is initially zero. Notice we used `requires_grade=True` for our backprop to work. We get the number of features from the `x_train.shape[1]` so that the weights is a column vector with one weight per input dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a55c9",
   "metadata": {},
   "source": [
    "With all the groundwork laid we can begin training our model! Using Batch Gradient Descent (BGD) with autodiff to compute the gradients we'll use the full training set at each training step.\n",
    "\n",
    "We'll calculate log using the squared mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73ffb2",
   "metadata": {},
   "source": [
    "#### Linear Regression Using PyTorch's Low-Level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92484a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 16.158458709716797\n",
      "Epoch 2/20, Loss: 4.879374027252197\n",
      "Epoch 3/20, Loss: 2.255225896835327\n",
      "Epoch 4/20, Loss: 1.3307636976242065\n",
      "Epoch 5/20, Loss: 0.9680691957473755\n",
      "Epoch 6/20, Loss: 0.8142675757408142\n",
      "Epoch 7/20, Loss: 0.741704523563385\n",
      "Epoch 8/20, Loss: 0.7020700573921204\n",
      "Epoch 9/20, Loss: 0.6765917539596558\n",
      "Epoch 10/20, Loss: 0.6577964425086975\n",
      "Epoch 11/20, Loss: 0.6426150798797607\n",
      "Epoch 12/20, Loss: 0.6297222971916199\n",
      "Epoch 13/20, Loss: 0.6184941530227661\n",
      "Epoch 14/20, Loss: 0.6085968017578125\n",
      "Epoch 15/20, Loss: 0.5998216271400452\n",
      "Epoch 16/20, Loss: 0.5920186638832092\n",
      "Epoch 17/20, Loss: 0.5850691199302673\n",
      "Epoch 18/20, Loss: 0.578873336315155\n",
      "Epoch 19/20, Loss: 0.5733453631401062\n",
      "Epoch 20/20, Loss: 0.5684100389480591\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.4\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = x_train @ w + b\n",
    "    loss = ((y_pred - y_train) ** 2).mean()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        b -= learning_rate * b.grad\n",
    "        w -= learning_rate * w.grad\n",
    "        b.grad.zero_()\n",
    "        w.grad.zero_()\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fc0db",
   "metadata": {},
   "source": [
    "The forward pass is the `y_pred` predictions calculation and the mean squared error `loss`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebeece",
   "metadata": {},
   "source": [
    "The `loss.backward()` method performs the backpropagation against the loss function (is loss a tensor here?) **this is autograd**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c155a7b",
   "metadata": {},
   "source": [
    "Gradient descent is then performed against the bias and weights as described above in a method where the gradients are not affected by the descent step operations. Gradients are then zeroed out in-place with `.zero_()` before the next epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312d3ed",
   "metadata": {},
   "source": [
    "Making predictions with our trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "818c17ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8916],\n",
       "        [1.6480],\n",
       "        [2.6577]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = x_test[:3]\n",
    "with torch.no_grad():\n",
    "    y_pred = x_new @ w + b\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860da658",
   "metadata": {},
   "source": [
    "> Best practice is using `torch.no_grad()` context during inference. there is not need to keep track of the graph and it's computationally cheaper in cycles and memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7a204",
   "metadata": {},
   "source": [
    "#### Linear Regression Using PyTorch's High-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20b5bf",
   "metadata": {},
   "source": [
    "The above process is intensive in it's verbosity, we have a lot of control but must also do a lot of work. We can utilize the higher level abstractions available in PyTorch `toch.nn.Linear` class to achieve our goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f486ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Linear(in_features=n_features, out_features=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432618da",
   "metadata": {},
   "source": [
    "The `nn.Linear` subclass is a module from `nn.Module` class, modules can be used as building blocks for more complex operations. \n",
    "\n",
    "The `nn.Linear` module contains a `bias` vector with one bias term per neuron, and a `weight` matrix with one row per neuron and one column per input dimension, which is the tranpose of the weight matrix presented in computing outputs of a fully connected layer equation (rows and cols flipped).\n",
    "\n",
    "In this example the model has a single neuron `out_features=1` the `bias` vector therefore contains a single term and the `weight` matrix is a single row. \n",
    "\n",
    "These parameters are accessible as attributes of the `nn.Linear` module `model` we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b67b36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3117], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ca47202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49bfdf3",
   "metadata": {},
   "source": [
    "So this seems to have done the `w = torch.tensor(...)` work we did earlier by hand as it were. The weights seem pretty random but the bias is not zero which we had used before. I wonder if that can be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "504229a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3117], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62f47f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
      "       requires_grad=True))\n",
      "('bias', Parameter containing:\n",
      "tensor([0.3117], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0294729a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4718],\n",
       "        [ 0.1131]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162309e",
   "metadata": {},
   "source": [
    "The above function call computes the internal `forward()` method of the model. In this case it's `x @ self.weight.T + self.bias` for linear regression.\n",
    "\n",
    "With the model in hand we have a way to perform inference i.e. making predictions by calling the `forward()` method by using the model as a function: `model(x_train[:2])`.\n",
    "\n",
    "What's left now is a way to iteratively adjust the parameters with respect to the loss function in order to minimize the loss over time. Optimization. \n",
    "\n",
    "PyTorch includes a Stochastic Gradient Descent (SGD) optimizer that we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "956100a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c79c4d",
   "metadata": {},
   "source": [
    "Great, so the optimizer has been initialized with the model's parameters (weights and bias) and the learning rate.\n",
    "\n",
    "For the loss we can use mean squared error again but instead of calculating ourselves we can use a another `nn` built-in method.\n",
    "\n",
    "In PyTorch the loss function is usually referred to as the *criterion* to distinguish the function from its output `loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4636ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a00827",
   "metadata": {},
   "source": [
    "With everything in place, our prediction, optimizer, and loss function we can build our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f206325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bgd(model, optimizer, criterion, x_train, y_train, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        y_pred = model(x_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd7bdc",
   "metadata": {},
   "source": [
    "- The `optimizer.step()` function is equivalent to our previous example manipulating `b` and `w` inside of the `no_grad()` \n",
    "- Same with the `zero_grad` zeroing out the gradients for each parameter we initialized the optimizer with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99e24287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss 4.337850093841553\n",
      "Epoch 2/20, Loss 0.7802939414978027\n",
      "Epoch 3/20, Loss 0.6253842115402222\n",
      "Epoch 4/20, Loss 0.6060435175895691\n",
      "Epoch 5/20, Loss 0.5956299304962158\n",
      "Epoch 6/20, Loss 0.587356686592102\n",
      "Epoch 7/20, Loss 0.5802990198135376\n",
      "Epoch 8/20, Loss 0.5741382837295532\n",
      "Epoch 9/20, Loss 0.5687101483345032\n",
      "Epoch 10/20, Loss 0.5639079213142395\n",
      "Epoch 11/20, Loss 0.5596511363983154\n",
      "Epoch 12/20, Loss 0.5558737516403198\n",
      "Epoch 13/20, Loss 0.5525194406509399\n",
      "Epoch 14/20, Loss 0.5495392084121704\n",
      "Epoch 15/20, Loss 0.5468899607658386\n",
      "Epoch 16/20, Loss 0.5445339679718018\n",
      "Epoch 17/20, Loss 0.5424376726150513\n",
      "Epoch 18/20, Loss 0.5405715703964233\n",
      "Epoch 19/20, Loss 0.5389097332954407\n",
      "Epoch 20/20, Loss 0.5374288558959961\n"
     ]
    }
   ],
   "source": [
    "train_bgd(model, optimizer, mse, x_train, y_train, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89193068",
   "metadata": {},
   "source": [
    "And the model is trained! Making predictions can be done by calling the function as we saw earlier, but inside of a `torch.no_grad()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "582576f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = x_test[:3]\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38f6b9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8061],\n",
       "        [1.7116],\n",
       "        [2.6973]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3deb63",
   "metadata": {},
   "source": [
    "I got my above question answered about initializing the model. The `nn.Linear` module uses a uniform random distribution from - sqrt(2) / 4 to + sqrt(2) / 4 for both the weights and bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9abf501",
   "metadata": {},
   "source": [
    "## Implementing a Regression MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa86c0e9",
   "metadata": {},
   "source": [
    "Lots of neural networks are just stacks of modules, one very useful big stack of modules is the `nn.Sequential` module which we can use to make our MLP with two hidden layers and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c90db746",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7171db7",
   "metadata": {},
   "source": [
    "- ReLU is the activation function we've seen before and it acts item-wise and requires no parameters it will input and output the same dimensions\n",
    "- The hidden layers are `nn.Linear` instances and the first layer input must match the number of features for our data. The output is a tunabel hyperparameter, in this case 50.\n",
    "- The second hidden layer shows how the input must match the output of the previous layer. It's common to use the same output for each hidden layer.\n",
    "- The final output layer must match the output of the previous to its input *and* match its output size to the dimension of our target. Since our target is 1-Dimensional (a housing price prediction) we have a single output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b22d302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss 5.045480251312256\n",
      "Epoch 2/20, Loss 2.0523128509521484\n",
      "Epoch 3/20, Loss 1.0039883852005005\n",
      "Epoch 4/20, Loss 0.8570139408111572\n",
      "Epoch 5/20, Loss 0.7740675210952759\n",
      "Epoch 6/20, Loss 0.7225848436355591\n",
      "Epoch 7/20, Loss 0.6893726587295532\n",
      "Epoch 8/20, Loss 0.6669033169746399\n",
      "Epoch 9/20, Loss 0.6507739424705505\n",
      "Epoch 10/20, Loss 0.6383934020996094\n",
      "Epoch 11/20, Loss 0.6281994581222534\n",
      "Epoch 12/20, Loss 0.6193398833274841\n",
      "Epoch 13/20, Loss 0.6113173365592957\n",
      "Epoch 14/20, Loss 0.6038705110549927\n",
      "Epoch 15/20, Loss 0.5968307852745056\n",
      "Epoch 16/20, Loss 0.5901119112968445\n",
      "Epoch 17/20, Loss 0.5836467742919922\n",
      "Epoch 18/20, Loss 0.5774063467979431\n",
      "Epoch 19/20, Loss 0.5713555216789246\n",
      "Epoch 20/20, Loss 0.565444827079773\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "mse = nn.MSELoss()\n",
    "train_bgd(model, optimizer, mse, x_train, y_train, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0cc8d",
   "metadata": {},
   "source": [
    "Woohoo! My first neural network trained with PyTorch! I am going to tell all of my friends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec0f3c",
   "metadata": {},
   "source": [
    "## Implementing Mini-Batch Gradient Descent Using DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc34900",
   "metadata": {},
   "source": [
    "Thus far we've been training against the full training set `x_train` and `y_train` without breaking it up into more managable bathces. This changes now, using the `DataLoader` class from `torch.utils.data` we will be able to break the training set up and even shuffle it with `shuffle=True`.\n",
    "\n",
    "Because our data `x_train` and `y_train` are tensors we'll use the `TensorDataset` class to wrap them in the approrpiate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ea55f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=21, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c806d9",
   "metadata": {},
   "source": [
    "Moving a model to the GPU loads all of its parameters to the GPU/Accelerator RAM. At the start of each iteration during training we copy each batch to the device. We use the `to()` method as we did with tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00dd6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 1)\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffcc5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13d4dc",
   "metadata": {},
   "source": [
    "> Best practice to initialized optimizers **after** you have moved the model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3955224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {mean_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a9b5a",
   "metadata": {},
   "source": [
    "Very similar to our previous with some extra logic for the batches. Notice the `model.train()` at the start this sets the model into **training mode** which is distinct from an evaluation mode with `model.eval()` so calling these functions essentially just flips a boolean within the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56ae5d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.7774\n",
      "Epoch 2/20, Loss: 0.4419\n",
      "Epoch 3/20, Loss: 0.4120\n",
      "Epoch 4/20, Loss: 0.3883\n",
      "Epoch 5/20, Loss: 0.3787\n",
      "Epoch 6/20, Loss: 0.3810\n",
      "Epoch 7/20, Loss: 0.3799\n",
      "Epoch 8/20, Loss: 0.3641\n",
      "Epoch 9/20, Loss: 0.3537\n",
      "Epoch 10/20, Loss: 0.3539\n",
      "Epoch 11/20, Loss: 0.3512\n",
      "Epoch 12/20, Loss: 0.3586\n",
      "Epoch 13/20, Loss: 0.3448\n",
      "Epoch 14/20, Loss: 0.3337\n",
      "Epoch 15/20, Loss: 0.3544\n",
      "Epoch 16/20, Loss: 0.3500\n",
      "Epoch 17/20, Loss: 0.3612\n",
      "Epoch 18/20, Loss: 0.3519\n",
      "Epoch 19/20, Loss: 0.3432\n",
      "Epoch 20/20, Loss: 0.3921\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, mse, train_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ec142",
   "metadata": {},
   "source": [
    "##### Some minor perfomance tweaks to the DataLoader:\n",
    "- use `pin_memory=True` when creating the data loader, allocates the data in page-locked memory\n",
    "- allow the CPU to prefetch batches my adjusting `num_workers` `prefetch_factor` and `persistent_workers=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4cf70e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
